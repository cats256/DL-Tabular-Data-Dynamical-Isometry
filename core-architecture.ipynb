{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is recommended to read the explanation in README.md first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import logit\n",
    "from scipy.stats import norm\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import layers, models\n",
    "from keras.datasets import mnist\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchinfo import summary\n",
    "\n",
    "from sklearn.model_selection import train_test_split, LeaveOneOut, cross_val_predict, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, PowerTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, log_loss, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by https://arxiv.org/abs/1702.08591 (looks-linear init), https://arxiv.org/abs/1711.04735 (dynamical isometry), and https://openreview.net/forum?id=Fp7__phQszn (inductive bias of preserving original orientation of features with almost linear input-output mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLinearLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size, mode=\"default\"):\n",
    "        super(CustomLinearLayer, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size, bias=True)\n",
    "        nn.init.zeros_(self.linear.bias)\n",
    "        \n",
    "        if mode == \"init_zero\":\n",
    "            nn.init.zeros_(self.linear.weight)\n",
    "        elif mode == \"splits_inputs\":\n",
    "            self.splits_inputs()\n",
    "        elif mode == \"looks_linear_init\":\n",
    "            self.looks_linear_init()\n",
    "\n",
    "    def looks_linear_init(self):\n",
    "        with torch.no_grad():\n",
    "            size = self.linear.weight.size(0)\n",
    "            weight = torch.zeros(size, size)\n",
    "\n",
    "            indices = torch.arange(0, size, step=2)\n",
    "\n",
    "            weight[indices, indices] = 1\n",
    "            weight[indices, indices + 1] = -1\n",
    "            weight[indices + 1, indices] = -1\n",
    "            weight[indices + 1, indices + 1] = 1\n",
    "\n",
    "            self.linear.weight.copy_(weight)\n",
    "        \n",
    "        \"\"\" Example matrix: [\n",
    "            [1, -1, 0, 0],\n",
    "            [-1, 1, 0, 0],\n",
    "            [0, 0, 1, -1],\n",
    "            [0, 0, -1, 1]\n",
    "        ] \"\"\"\n",
    "            \n",
    "    def splits_inputs(self):\n",
    "        with torch.no_grad():\n",
    "            weight = torch.zeros(self.linear.out_features, self.linear.in_features)\n",
    "\n",
    "            for i in range(self.linear.in_features):\n",
    "                weight[2 * i, i] = 1\n",
    "                weight[2 * i + 1, i] = -1\n",
    "\n",
    "            self.linear.weight.copy_(weight)\n",
    "            \n",
    "        \"\"\" Example matrix: [\n",
    "            [1, 0, 0],\n",
    "            [-1, 0, 0],\n",
    "            [0, 1, 0],\n",
    "            [0, -1, 0],\n",
    "            [0, 0, 1],\n",
    "            [0, 0, -1]\n",
    "        ] \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar in logic to DenseNet \"of connecting each layer to every other layer in a feed-forward fashion\", where \"for each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers\" https://arxiv.org/abs/1711.04735 but for MLPs or fully connected neural networks. Current, naive implementations of DenseNet consumes quadratic memory with respect to depth but since feature maps are reused almost everywhere, through some implementation tricks DenseNet can also be implemented in linear memory https://arxiv.org/abs/1707.06990. This will be worked on later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDenseNet(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, output_size):\n",
    "        super(TabularDenseNet, self).__init__()\n",
    "        self.activation = nn.Softplus()\n",
    "        \n",
    "        layer_size = input_size * 2\n",
    "        self.res_layer = CustomLinearLayer(input_size, layer_size, mode=\"splits_inputs\")\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.layers.append(CustomLinearLayer(layer_size, layer_size, mode=\"looks_linear_init\"))\n",
    "            layer_size *= 2\n",
    "            \n",
    "        self.last_layer = CustomLinearLayer(layer_size, output_size, mode=\"init_zero\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = [self.activation(self.res_layer(x))]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            concatenated_outputs = torch.cat(outputs, dim=1)\n",
    "            outputs.append(self.activation(layer(concatenated_outputs)))\n",
    "\n",
    "        concatenated_outputs = torch.cat(outputs, dim=1)\n",
    "        return self.last_layer(concatenated_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Loss below with L1 and L2 regularization is only recommended to use with SGD optimizer. There are issues with adaptive gradient algorithm when doing navive L1/L2 regularization as mentioned in this paper https://arxiv.org/abs/1711.05101. I'll find some time to address this issue later. L1 regularization is particularly beneficial as it is rotationally invariant and robust against uninformative features, which are also the inductive biases that contribute to tree-based models strong performance on tabular data https://openreview.net/forum?id=Fp7__phQszn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, criterion, l1_lambda, l2_lambda):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.criterion = criterion\n",
    "        self.l1_lambda = l1_lambda\n",
    "        self.l2_lambda = l2_lambda\n",
    "\n",
    "    def forward(self, outputs, labels, model):\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        \n",
    "        l1_norm = sum(p.abs().sum() for name, p in model.named_parameters() if 'bias' not in name)\n",
    "        l2_norm = sum(p.pow(2.0).sum() for name, p in model.named_parameters() if 'bias' not in name)\n",
    "        \n",
    "        loss += self.l1_lambda * l1_norm + self.l2_lambda * l2_norm\n",
    "        return loss"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
