{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update: Showed some good initial result but training is infeasible due to limited expressivity from 2-layer neural network even with ultra wide width. This can be expected\n",
    "\n",
    "Demonstrated some promising result. Concept is similar to gradient boosting, where model is trained with few neurons, then new neurons are added and original neurons are freezed and continue training. Freezing prevents original neurons from working with new neurons to memorize training data, rather than generalizing through robust patterns. Also utilizes \"looks linear\" init which helps with speeding up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerGradientBoosting(nn.Module):\n",
    "    def splits_inputs_init(self, layer):\n",
    "        with torch.no_grad():\n",
    "            weight = torch.zeros(layer.out_features, layer.in_features)\n",
    "\n",
    "            for i in range(layer.in_features):\n",
    "                weight[2 * i, i] = 1\n",
    "                weight[2 * i + 1, i] = -1\n",
    "\n",
    "            layer.weight.copy_(weight)\n",
    "            \n",
    "        \"\"\" Example matrix: [\n",
    "            [1, 0, 0],\n",
    "            [-1, 0, 0],\n",
    "            [0, 1, 0],\n",
    "            [0, -1, 0],\n",
    "            [0, 0, 1],\n",
    "            [0, 0, -1]\n",
    "        ] \"\"\"\n",
    "        \n",
    "    def __init__(self, input_size, output_size, num_neurons):\n",
    "        super(TwoLayerGradientBoosting, self).__init__()\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.fc1_trainable = nn.Linear(input_size, input_size * 2 + num_neurons)\n",
    "        self.fc2_trainable = nn.Linear(input_size * 2 + num_neurons, output_size)\n",
    "        \n",
    "        self.splits_inputs_init(self.fc1_trainable)\n",
    "        nn.init.zeros_(self.fc1_trainable.bias)\n",
    "\n",
    "        nn.init.zeros_(self.fc2_trainable.weight)\n",
    "        nn.init.zeros_(self.fc2_trainable.bias)\n",
    "\n",
    "        self.fc1_frozen = None\n",
    "        self.fc2_frozen = None\n",
    "\n",
    "    def grow_network(self, num_new_neurons):\n",
    "        if self.fc1_frozen is None:\n",
    "            self.fc1_frozen = self.fc1_trainable\n",
    "            self.fc2_frozen = self.fc2_trainable\n",
    "        else:\n",
    "            new_fc1_frozen = nn.Linear(self.input_size, self.fc1_frozen.out_features + self.fc1_trainable.out_features)\n",
    "            new_fc2_frozen = nn.Linear(self.fc1_frozen.out_features + self.fc1_trainable.out_features, self.output_size)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                new_fc1_frozen.weight[:self.fc1_frozen.out_features] = self.fc1_frozen.weight\n",
    "                new_fc1_frozen.bias[:self.fc1_frozen.out_features] = self.fc1_frozen.bias\n",
    "\n",
    "                new_fc1_frozen.weight[self.fc1_frozen.out_features:] = self.fc1_trainable.weight\n",
    "                new_fc1_frozen.bias[self.fc1_frozen.out_features:] = self.fc1_trainable.bias\n",
    "\n",
    "                new_fc2_frozen.weight[:, :self.fc1_frozen.out_features] = self.fc2_frozen.weight\n",
    "                new_fc2_frozen.weight[:, self.fc1_frozen.out_features:] = self.fc2_trainable.weight\n",
    "\n",
    "                new_fc2_frozen.bias = torch.nn.Parameter(self.fc2_frozen.bias + self.fc2_trainable.bias)\n",
    "\n",
    "            self.fc1_frozen = new_fc1_frozen\n",
    "            self.fc2_frozen = new_fc2_frozen\n",
    "\n",
    "        self.fc1_trainable = nn.Linear(self.input_size, self.input_size * 2 + num_new_neurons)\n",
    "        self.fc2_trainable = nn.Linear(self.input_size * 2 + num_new_neurons, self.output_size)\n",
    "        \n",
    "        self.splits_inputs_init(self.fc1_trainable)\n",
    "        nn.init.zeros_(self.fc2_trainable.bias)\n",
    "\n",
    "        nn.init.zeros_(self.fc2_trainable.weight)\n",
    "        nn.init.zeros_(self.fc2_trainable.bias)\n",
    "\n",
    "        self.fc1_frozen.to(device)\n",
    "        self.fc2_frozen.to(device)\n",
    "\n",
    "        self.fc1_trainable.to(device)\n",
    "        self.fc2_trainable.to(device)\n",
    "\n",
    "        for param in self.fc1_frozen.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in self.fc2_frozen.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.fc1_frozen is None:  \n",
    "            x = self.activation(self.fc1_trainable(x))\n",
    "            return self.fc2_trainable(x)\n",
    "        \n",
    "        x_frozen = self.activation(self.fc1_frozen(x))\n",
    "        x_frozen = self.fc2_frozen(x_frozen)\n",
    "        \n",
    "        x = self.activation(self.fc1_trainable(x))\n",
    "        x = self.fc2_trainable(x)\n",
    "        return x_frozen + x\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
