{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternate architecture with Gated Linear Unit (GLU). This should also be able to achieve dynamical isometry as it achieves identity input-output mapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGLU(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(CustomGLU, self).__init__()\n",
    "        self.linear_main = nn.Linear(input_dim, output_dim)\n",
    "        self.linear_gate = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.linear_main.weight.copy_(torch.eye(output_dim, input_dim))\n",
    "            self.linear_main.bias.fill_(0)\n",
    "\n",
    "            self.linear_gate.weight.fill_(0)\n",
    "            self.linear_gate.bias.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = self.linear_main(x)\n",
    "        b = self.linear_gate(x)\n",
    "\n",
    "        return a * torch.sigmoid(b) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDenseNet(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, output_size):\n",
    "        super(TabularDenseNet, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            self.layers.append(CustomGLU(input_size, input_size))\n",
    "            input_size *= 2\n",
    "            \n",
    "        self.last_layer = nn.Linear(input_size, output_size)\n",
    "        with torch.no_grad():\n",
    "            nn.init.zeros_(self.last_layer.weight)\n",
    "            nn.init.zeros_(self.last_layer.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        outputs = [x]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            concatenated_outputs = torch.cat(outputs, dim=1)\n",
    "            outputs.append(layer(concatenated_outputs))\n",
    "\n",
    "        concatenated_outputs = torch.cat(outputs, dim=1)\n",
    "        return self.last_layer(concatenated_outputs)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
