{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Promising result in terms of generalization so far but it's a bit hard to train as it requires growing the network multiple times and adjusting the learning rate accordingly, rather than just growing one big model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.datasets import california_housing\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import sys\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.datasets import mnist\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import logit\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.model_selection import LeaveOneOut, cross_val_predict, StratifiedKFold\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLinearLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size, init=\"looks_linear\"):\n",
    "        super(CustomLinearLayer, self).__init__()\n",
    "        if init == \"zero\":\n",
    "            self.linear = nn.Linear(input_size, output_size, bias=True)\n",
    "\n",
    "            nn.init.zeros_(self.linear.bias)\n",
    "            nn.init.zeros_(self.linear.weight)\n",
    "        elif init == \"looks_linear\":\n",
    "            if input_size * 2 != output_size:\n",
    "                print(\"Output size must be twice that of input size\")\n",
    "                return\n",
    "                \n",
    "            self.linear = nn.Linear(input_size, output_size, bias=True)\n",
    "            nn.init.zeros_(self.linear.bias)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                weight = torch.zeros(input_size * 2, input_size)\n",
    "\n",
    "                for i in range(self.linear.in_features):\n",
    "                    weight[2 * i, i] = 1\n",
    "                    weight[2 * i + 1, i] = -1\n",
    "\n",
    "                self.linear.weight.copy_(weight)\n",
    "                nn.init.zeros_(self.linear.bias)\n",
    "                \n",
    "            \"\"\" Example matrix: [\n",
    "                [1, 0, 0],\n",
    "                [-1, 0, 0],\n",
    "                [0, 1, 0],\n",
    "                [0, -1, 0],\n",
    "                [0, 0, 1],\n",
    "                [0, 0, -1]\n",
    "            ] \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoostedResnet(nn.Module):        \n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(GradientBoostedResnet, self).__init__()\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        self.first_layers = nn.ModuleList()\n",
    "        self.last_layers = nn.ModuleList()\n",
    "\n",
    "        self.starting_layer = CustomLinearLayer(input_size, output_size, init=\"zero\")\n",
    "        self.layer_size = input_size\n",
    "\n",
    "    def grow_network(self):\n",
    "        if len(self.first_layers) == 0:\n",
    "            for param in self.starting_layer.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            for param in self.first_layers[-1].parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            for param in self.last_layers[-1].parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        self.first_layers.append(CustomLinearLayer(self.layer_size, self.layer_size * 2, init=\"looks_linear\").to(device))\n",
    "        self.last_layers.append(CustomLinearLayer(self.layer_size * 2, self.output_size, init=\"zero\").to(device))\n",
    "        self.layer_size += self.layer_size * 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = [x]\n",
    "        for layer in self.first_layers:\n",
    "            concatenated_outputs = torch.cat(outputs, dim=1)\n",
    "            outputs.append(self.activation(layer(concatenated_outputs)))\n",
    "\n",
    "        result = self.starting_layer(outputs[0])\n",
    "        for i in range(len(self.last_layers)):\n",
    "            result += self.last_layers[i](outputs[i + 1])\n",
    "\n",
    "        return result"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
