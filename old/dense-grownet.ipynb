{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":408,"sourceType":"datasetVersion","datasetId":180},{"sourceId":472,"sourceType":"datasetVersion","datasetId":222},{"sourceId":668,"sourceType":"datasetVersion","datasetId":308},{"sourceId":57419,"sourceType":"datasetVersion","datasetId":37691},{"sourceId":2789260,"sourceType":"datasetVersion","datasetId":1703281},{"sourceId":6724823,"sourceType":"datasetVersion","datasetId":3873965}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport keras\nimport tensorflow as tf\nfrom keras import layers\nfrom keras import models\nfrom keras.datasets import california_housing\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import TensorDataset, DataLoader\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchinfo import summary\nfrom torch.optim.lr_scheduler import StepLR\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.preprocessing import StandardScaler\n\nimport sys\nimport math\n\nimport numpy as np\nimport math\n\nimport numpy as np\nimport keras\nimport tensorflow as tf\nfrom keras import layers\nfrom keras import models\nfrom keras.datasets import mnist\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import TensorDataset, DataLoader\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchinfo import summary\nfrom torch.optim.lr_scheduler import StepLR\n\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import logit\nfrom sklearn.preprocessing import StandardScaler, PowerTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss, accuracy_score\nfrom sklearn.model_selection import LeaveOneOut, cross_val_predict, StratifiedKFold\nfrom scipy.stats import norm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-02T01:09:25.112078Z","iopub.execute_input":"2024-10-02T01:09:25.112878Z","iopub.status.idle":"2024-10-02T01:09:42.513847Z","shell.execute_reply.started":"2024-10-02T01:09:25.112836Z","shell.execute_reply":"2024-10-02T01:09:42.512584Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-10-02T01:09:42.515753Z","iopub.execute_input":"2024-10-02T01:09:42.516453Z","iopub.status.idle":"2024-10-02T01:09:42.521747Z","shell.execute_reply.started":"2024-10-02T01:09:42.516401Z","shell.execute_reply":"2024-10-02T01:09:42.520568Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def calculate_metrics(model, loader):\n    model.eval()\n        \n    with torch.no_grad():\n        outputs = model(loader.val_data_tensor, loader.raw_output_val)\n        \n        _, preds = torch.max(outputs, 1)\n        print(outputs.shape)\n        \n        all_preds = preds.cpu().numpy()\n        all_labels = loader.val_labels_tensor.cpu().numpy()\n    \n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    \n    return accuracy, f1","metadata":{"execution":{"iopub.status.busy":"2024-10-02T04:06:11.290419Z","iopub.execute_input":"2024-10-02T04:06:11.290843Z","iopub.status.idle":"2024-10-02T04:06:11.297547Z","shell.execute_reply.started":"2024-10-02T04:06:11.290797Z","shell.execute_reply":"2024-10-02T04:06:11.296343Z"},"trusted":true},"execution_count":535,"outputs":[]},{"cell_type":"code","source":"class CustomDataLoader:\n    def __init__(self, features, labels, batch_size=1, validation_size=0.0, raw_output=None):\n        train_indices, val_indices, train_labels, val_labels = train_test_split(\n            range(len(labels)), labels, test_size=0.2, stratify=labels, random_state=42\n        )\n    \n        train_data = features[train_indices]\n        val_data = features[val_indices]\n\n        self.train_data_tensor = torch.tensor(train_data).float().to(device)\n        self.train_labels_tensor = torch.tensor(train_labels).long().to(device)\n        \n        if raw_output is None:\n            self.raw_output_train = None\n            self.raw_output_val = None\n        else:\n            self.raw_output_train = raw_output[train_indices].clone().detach().float().to(device)\n            self.raw_output_val = raw_output[val_indices].clone().detach().float().to(device)\n\n        self.val_data_tensor = torch.tensor(val_data).float().to(device)\n        self.val_labels_tensor = torch.tensor(val_labels).long().to(device)\n\n        train_dataset = TensorDataset(self.train_data_tensor, self.train_labels_tensor)\n        val_dataset = TensorDataset(self.val_data_tensor, self.val_labels_tensor)\n\n        self.train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n        self.val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n    def get_train_loader(self):\n        return self.train_loader\n    \n    def get_val_loader(self):\n        return self.val_loader","metadata":{"execution":{"iopub.status.busy":"2024-10-02T03:47:14.621833Z","iopub.execute_input":"2024-10-02T03:47:14.622320Z","iopub.status.idle":"2024-10-02T03:47:14.632838Z","shell.execute_reply.started":"2024-10-02T03:47:14.622275Z","shell.execute_reply":"2024-10-02T03:47:14.631837Z"},"trusted":true},"execution_count":474,"outputs":[]},{"cell_type":"code","source":"def evaluate_model(model, custom_train_loader, criterion, optimizer, num_epochs, num_features, batch_size, patience=None):\n    best_val_loss, early_val_accuracy, early_train_loss = float('inf'), 0.0, float('inf')\n    best_epoch = 0\n    patience_counter = 0\n\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        for i in range(0, custom_train_loader.train_data_tensor.size(0), batch_size):\n            inputs, labels = custom_train_loader.train_data_tensor[i:i + batch_size], custom_train_loader.train_labels_tensor[i:i + batch_size]\n            optimizer.zero_grad()\n            \n            outputs = None\n            if custom_train_loader.raw_output_train is None:\n                outputs = model(inputs.view(-1, num_features))\n            else:\n                outputs = model(inputs.view(-1, num_features), custom_train_loader.raw_output_train[i:i + batch_size])\n                \n            loss = criterion(outputs, labels, model)\n            running_loss += loss.item()\n    \n            loss.backward()\n            optimizer.step()\n\n        model.eval()\n        running_val_loss = 0.0\n\n        unregularized_criterion=nn.CrossEntropyLoss()\n        with torch.no_grad():\n            for i in range(0, custom_train_loader.val_data_tensor.size(0), batch_size):\n                inputs, labels = custom_train_loader.val_data_tensor[i:i + batch_size], custom_train_loader.val_labels_tensor[i:i + batch_size]\n                inputs, labels = inputs.to(device), labels.to(device)\n                \n                if custom_train_loader.raw_output_train is None:\n                    outputs = model(inputs.view(-1, num_features))\n                else:\n                    outputs = model(inputs.view(-1, num_features), custom_train_loader.raw_output_val[i:i + batch_size])\n\n                val_loss = unregularized_criterion(outputs, labels)\n                running_val_loss += val_loss.item()\n\n            avg_val_loss = running_val_loss / (len(custom_train_loader.get_val_loader()))\n\n            val_accuracy, val_f1 = calculate_metrics(model, custom_train_loader)\n\n            if (epoch + 1) % 1 == 0:\n                print(f'Epoch {epoch+1}, Loss: {running_loss / (len(custom_train_loader.get_train_loader()))}')\n                print(f'Validation Loss: {avg_val_loss}')\n                print(f'Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f}')\n                print()\n\n            if patience is None:\n                continue\n            elif avg_val_loss < best_val_loss:\n                best_val_loss = avg_val_loss\n                early_val_accuracy = val_accuracy\n                early_train_loss = running_loss / len(custom_train_loader.get_train_loader())\n                best_epoch = epoch\n                patience_counter = 0\n            else:\n                patience_counter += 1\n\n            if patience_counter >= patience:\n                print(f'Early stopping triggered. Best validation loss at epoch {best_epoch+1}: {best_val_loss:.4f}')\n                print(f'Val Accuracy: {early_val_accuracy:.4f}')\n                print(f'Early Train Loss: {early_train_loss}')\n                break","metadata":{"execution":{"iopub.status.busy":"2024-10-02T03:47:14.794058Z","iopub.execute_input":"2024-10-02T03:47:14.794807Z","iopub.status.idle":"2024-10-02T03:47:14.809171Z","shell.execute_reply.started":"2024-10-02T03:47:14.794763Z","shell.execute_reply":"2024-10-02T03:47:14.808082Z"},"trusted":true},"execution_count":475,"outputs":[]},{"cell_type":"code","source":"class CustomLoss(nn.Module):\n    def __init__(self, criterion, l1_lambda, l2_lambda):\n        super(CustomLoss, self).__init__()\n        self.criterion = criterion\n        self.l1_lambda = l1_lambda\n        self.l2_lambda = l2_lambda\n\n    def forward(self, outputs, labels, model):\n        loss = self.criterion(outputs, labels)\n        \n        l1_norm = sum(p.abs().sum() for name, p in model.named_parameters() if 'bias' not in name)\n        l2_norm = sum(p.pow(2.0).sum() for name, p in model.named_parameters() if 'bias' not in name)\n        \n        loss += self.l1_lambda * l1_norm + self.l2_lambda * l2_norm\n        return loss","metadata":{"execution":{"iopub.status.busy":"2024-10-02T03:47:15.021543Z","iopub.execute_input":"2024-10-02T03:47:15.022294Z","iopub.status.idle":"2024-10-02T03:47:15.029693Z","shell.execute_reply.started":"2024-10-02T03:47:15.022235Z","shell.execute_reply":"2024-10-02T03:47:15.028407Z"},"trusted":true},"execution_count":476,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/adult-income-dataset/adult.csv')\ndata = data.dropna()\n\nX = data.drop([\"income\"], axis=1)\ny = data[\"income\"]\n\nX = pd.get_dummies(X, drop_first=True)\nfor col in X.columns:\n    if (X[col] > 0).all():\n        X[col] = np.log(X[col])\n\nprint(X.shape)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T03:47:15.408840Z","iopub.execute_input":"2024-10-02T03:47:15.409315Z","iopub.status.idle":"2024-10-02T03:47:15.631621Z","shell.execute_reply.started":"2024-10-02T03:47:15.409243Z","shell.execute_reply":"2024-10-02T03:47:15.630552Z"},"trusted":true},"execution_count":477,"outputs":[{"name":"stdout","text":"(48842, 100)\n","output_type":"stream"}]},{"cell_type":"code","source":"x_scaler = StandardScaler()\nx_scaled = x_scaler.fit_transform(X)\n\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\nprint(y_encoded)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T03:47:15.821475Z","iopub.execute_input":"2024-10-02T03:47:15.821891Z","iopub.status.idle":"2024-10-02T03:47:16.071091Z","shell.execute_reply.started":"2024-10-02T03:47:15.821852Z","shell.execute_reply":"2024-10-02T03:47:16.069801Z"},"trusted":true},"execution_count":478,"outputs":[{"name":"stdout","text":"[0 0 1 ... 0 0 1]\n","output_type":"stream"}]},{"cell_type":"code","source":"class CustomLinearLayer(nn.Module):\n    def __init__(self, input_size, output_size, init=\"looks_linear\"):\n        super(CustomLinearLayer, self).__init__()\n        self.linear = nn.Linear(input_size, output_size, bias=True)\n        nn.init.zeros_(self.linear.bias)\n\n        if init == \"zero\":\n            nn.init.zeros_(self.linear.weight)\n        elif init == \"looks_linear\":\n            if input_size * 2 != output_size:\n                raise ValueError(\"Output size must be twice that of input size\")\n                \n            with torch.no_grad():\n                weight = torch.zeros(input_size * 2, input_size)\n\n                for i in range(self.linear.in_features):\n                    weight[2 * i, i] = 1\n                    weight[2 * i + 1, i] = -1\n\n                self.linear.weight.copy_(weight)\n                nn.init.zeros_(self.linear.bias)\n                \n            \"\"\" Example matrix: [\n                [1, 0, 0],\n                [-1, 0, 0],\n                [0, 1, 0],\n                [0, -1, 0],\n                [0, 0, 1],\n                [0, 0, -1]\n            ] \"\"\"\n\n    def forward(self, x):\n        return self.linear(x)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T03:50:53.239548Z","iopub.execute_input":"2024-10-02T03:50:53.239988Z","iopub.status.idle":"2024-10-02T03:50:53.249758Z","shell.execute_reply.started":"2024-10-02T03:50:53.239949Z","shell.execute_reply":"2024-10-02T03:50:53.248482Z"},"trusted":true},"execution_count":491,"outputs":[]},{"cell_type":"code","source":"class DenseGrowNet(nn.Module):        \n    def __init__(self, input_size, output_size, is_first_model):\n        super(DenseGrowNet, self).__init__()\n        self.activation = nn.ReLU()\n        \n        self.input_size = input_size\n        self.output_size = output_size\n        \n        self.is_first_model = is_first_model\n                        \n        if is_first_model:\n            self.first_layer = CustomLinearLayer(input_size, output_size, init=\"zero\")\n        else:\n            self.first_layer = CustomLinearLayer(input_size, input_size * 2, init=\"looks_linear\")\n            self.last_layer = CustomLinearLayer(input_size * 2, output_size, init=\"zero\")\n\n    def forward(self, x, prev_output=None):\n        if self.is_first_model and prev_output is not None:\n            raise ValueError(\"This is the first model and prev_output is passed in\")\n        if not self.is_first_model and prev_output is None:\n            raise ValueError(\"This is not the first model and prev_output is not passed in\")\n        \n        if prev_output is None:\n            return self.first_layer(x)\n        \n        x = self.activation(self.first_layer(x))\n        return self.last_layer(x) + prev_output","metadata":{"execution":{"iopub.status.busy":"2024-10-02T04:11:37.949176Z","iopub.execute_input":"2024-10-02T04:11:37.949628Z","iopub.status.idle":"2024-10-02T04:11:37.959901Z","shell.execute_reply.started":"2024-10-02T04:11:37.949589Z","shell.execute_reply":"2024-10-02T04:11:37.958419Z"},"trusted":true},"execution_count":547,"outputs":[]},{"cell_type":"code","source":"num_epochs = 100\nnum_features = 100\nnum_classes = 2\n\nmodel = DenseGrowNet(num_features, num_classes, True).to(device)\ncustom_train_loader = CustomDataLoader(x_scaled, y_encoded, batch_size=1024, validation_size=0.2, raw_output=None)\n\ncriterion = CustomLoss(criterion=nn.CrossEntropyLoss(), l1_lambda=0.0001 * 0, l2_lambda=0.0001 * 0)\noptimizer = optim.Adam(model.parameters(), lr=0.002)\n\nfor name, param in model.named_parameters():\n    break\n    print(f\"Layer: {name}\")\n    print(f\"Shape: {param.shape}\")\n    print(param)\n    \nsummary(model, input_size=(1, num_features))\nevaluate_model(model, custom_train_loader, criterion, optimizer, num_epochs, num_features, 1024, 5)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T03:52:02.144530Z","iopub.execute_input":"2024-10-02T03:52:02.144947Z","iopub.status.idle":"2024-10-02T03:52:07.381663Z","shell.execute_reply.started":"2024-10-02T03:52:02.144907Z","shell.execute_reply":"2024-10-02T03:52:07.380543Z"},"trusted":true},"execution_count":500,"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 0.5845796328324538\nValidation Loss: 0.5187402784824371\nVal Accuracy: 0.7475, Val F1-score: 0.7655\n\nEpoch 2, Loss: 0.49597448874742556\nValidation Loss: 0.4743054836988449\nVal Accuracy: 0.7804, Val F1-score: 0.7946\n\nEpoch 3, Loss: 0.45988934544416576\nValidation Loss: 0.444704607129097\nVal Accuracy: 0.8045, Val F1-score: 0.8150\n\nEpoch 4, Loss: 0.4332095170632387\nValidation Loss: 0.42139087021350863\nVal Accuracy: 0.8220, Val F1-score: 0.8292\n\nEpoch 5, Loss: 0.41191143026718724\nValidation Loss: 0.40256697237491607\nVal Accuracy: 0.8331, Val F1-score: 0.8379\n\nEpoch 6, Loss: 0.3946922413813762\nValidation Loss: 0.38731867372989653\nVal Accuracy: 0.8395, Val F1-score: 0.8424\n\nEpoch 7, Loss: 0.38075525714800906\nValidation Loss: 0.3749644041061401\nVal Accuracy: 0.8424, Val F1-score: 0.8437\n\nEpoch 8, Loss: 0.36946373337354416\nValidation Loss: 0.36494105458259585\nVal Accuracy: 0.8467, Val F1-score: 0.8470\n\nEpoch 9, Loss: 0.36029337919675386\nValidation Loss: 0.3567859947681427\nVal Accuracy: 0.8496, Val F1-score: 0.8489\n\nEpoch 10, Loss: 0.352818590708268\nValidation Loss: 0.35012570321559905\nVal Accuracy: 0.8523, Val F1-score: 0.8509\n\nEpoch 11, Loss: 0.34669904525463396\nValidation Loss: 0.3446622222661972\nVal Accuracy: 0.8546, Val F1-score: 0.8527\n\nEpoch 12, Loss: 0.34166516478245074\nValidation Loss: 0.34015966951847076\nVal Accuracy: 0.8562, Val F1-score: 0.8538\n\nEpoch 13, Loss: 0.3375042776266734\nValidation Loss: 0.33643168210983276\nVal Accuracy: 0.8558, Val F1-score: 0.8530\n\nEpoch 14, Loss: 0.33404865326025546\nValidation Loss: 0.33333105146884917\nVal Accuracy: 0.8562, Val F1-score: 0.8530\n\nEpoch 15, Loss: 0.33116574929310727\nValidation Loss: 0.330741086602211\nVal Accuracy: 0.8566, Val F1-score: 0.8531\n\nEpoch 16, Loss: 0.32875043612260085\nValidation Loss: 0.3285690754652023\nVal Accuracy: 0.8564, Val F1-score: 0.8526\n\nEpoch 17, Loss: 0.32671893177888334\nValidation Loss: 0.32674077451229094\nVal Accuracy: 0.8562, Val F1-score: 0.8522\n\nEpoch 18, Loss: 0.3250040977429121\nValidation Loss: 0.3251966416835785\nVal Accuracy: 0.8557, Val F1-score: 0.8514\n\nEpoch 19, Loss: 0.3235518542619852\nValidation Loss: 0.32388856410980227\nVal Accuracy: 0.8555, Val F1-score: 0.8511\n\nEpoch 20, Loss: 0.32231841102624553\nValidation Loss: 0.3227773606777191\nVal Accuracy: 0.8552, Val F1-score: 0.8506\n\nEpoch 21, Loss: 0.32126803810779864\nValidation Loss: 0.32183111906051637\nVal Accuracy: 0.8554, Val F1-score: 0.8507\n\nEpoch 22, Loss: 0.3203714933150854\nValidation Loss: 0.32102359235286715\nVal Accuracy: 0.8552, Val F1-score: 0.8504\n\nEpoch 23, Loss: 0.3196046688617804\nValidation Loss: 0.3203330963850021\nVal Accuracy: 0.8552, Val F1-score: 0.8503\n\nEpoch 24, Loss: 0.3189476300508548\nValidation Loss: 0.3197416812181473\nVal Accuracy: 0.8549, Val F1-score: 0.8500\n\nEpoch 25, Loss: 0.3183837647621448\nValidation Loss: 0.3192343652248383\nVal Accuracy: 0.8547, Val F1-score: 0.8498\n\nEpoch 26, Loss: 0.31789921338741595\nValidation Loss: 0.31879862844944\nVal Accuracy: 0.8546, Val F1-score: 0.8496\n\nEpoch 27, Loss: 0.3174823446151538\nValidation Loss: 0.31842395961284636\nVal Accuracy: 0.8549, Val F1-score: 0.8499\n\nEpoch 28, Loss: 0.31712334966048217\nValidation Loss: 0.31810145974159243\nVal Accuracy: 0.8551, Val F1-score: 0.8499\n\nEpoch 29, Loss: 0.3168139335436699\nValidation Loss: 0.3178236484527588\nVal Accuracy: 0.8552, Val F1-score: 0.8500\n\nEpoch 30, Loss: 0.3165470720865788\nValidation Loss: 0.31758415400981904\nVal Accuracy: 0.8549, Val F1-score: 0.8498\n\nEpoch 31, Loss: 0.3163167925981375\nValidation Loss: 0.31737756729125977\nVal Accuracy: 0.8549, Val F1-score: 0.8497\n\nEpoch 32, Loss: 0.31611797519219226\nValidation Loss: 0.317199245095253\nVal Accuracy: 0.8549, Val F1-score: 0.8497\n\nEpoch 33, Loss: 0.31594626643718815\nValidation Loss: 0.31704526841640474\nVal Accuracy: 0.8549, Val F1-score: 0.8497\n\nEpoch 34, Loss: 0.31579791811796337\nValidation Loss: 0.3169122487306595\nVal Accuracy: 0.8551, Val F1-score: 0.8498\n\nEpoch 35, Loss: 0.3156697146403484\nValidation Loss: 0.3167972803115845\nVal Accuracy: 0.8554, Val F1-score: 0.8500\n\nEpoch 36, Loss: 0.31555889814327925\nValidation Loss: 0.3166978806257248\nVal Accuracy: 0.8554, Val F1-score: 0.8500\n\nEpoch 37, Loss: 0.31546308749761337\nValidation Loss: 0.31661188304424287\nVal Accuracy: 0.8556, Val F1-score: 0.8502\n\nEpoch 38, Loss: 0.3153802072390532\nValidation Loss: 0.316537469625473\nVal Accuracy: 0.8557, Val F1-score: 0.8503\n\nEpoch 39, Loss: 0.31530849979473996\nValidation Loss: 0.3164730161428452\nVal Accuracy: 0.8557, Val F1-score: 0.8503\n\nEpoch 40, Loss: 0.31524641467974734\nValidation Loss: 0.316417196393013\nVal Accuracy: 0.8557, Val F1-score: 0.8503\n\nEpoch 41, Loss: 0.3151926352427556\nValidation Loss: 0.31636878848075867\nVal Accuracy: 0.8556, Val F1-score: 0.8502\n\nEpoch 42, Loss: 0.3151460114197853\nValidation Loss: 0.31632677614688876\nVal Accuracy: 0.8557, Val F1-score: 0.8503\n\nEpoch 43, Loss: 0.31510555362090087\nValidation Loss: 0.31629029512405393\nVal Accuracy: 0.8556, Val F1-score: 0.8502\n\nEpoch 44, Loss: 0.3150703876446455\nValidation Loss: 0.3162585854530334\nVal Accuracy: 0.8556, Val F1-score: 0.8502\n\nEpoch 45, Loss: 0.3150397791312291\nValidation Loss: 0.31623096466064454\nVal Accuracy: 0.8555, Val F1-score: 0.8500\n\nEpoch 46, Loss: 0.31501308389199084\nValidation Loss: 0.31620687544345855\nVal Accuracy: 0.8554, Val F1-score: 0.8499\n\nEpoch 47, Loss: 0.31498975096604764\nValidation Loss: 0.31618584394454957\nVal Accuracy: 0.8558, Val F1-score: 0.8503\n\nEpoch 48, Loss: 0.3149692867046747\nValidation Loss: 0.3161674290895462\nVal Accuracy: 0.8558, Val F1-score: 0.8503\n\nEpoch 49, Loss: 0.31495127616784513\nValidation Loss: 0.3161512970924377\nVal Accuracy: 0.8557, Val F1-score: 0.8502\n\nEpoch 50, Loss: 0.3149353838883914\nValidation Loss: 0.31613710820674895\nVal Accuracy: 0.8558, Val F1-score: 0.8503\n\nEpoch 51, Loss: 0.3149212889182262\nValidation Loss: 0.31612461507320405\nVal Accuracy: 0.8558, Val F1-score: 0.8503\n\nEpoch 52, Loss: 0.31490871615898913\nValidation Loss: 0.3161135822534561\nVal Accuracy: 0.8559, Val F1-score: 0.8504\n\nEpoch 53, Loss: 0.3148974462961539\nValidation Loss: 0.3161037892103195\nVal Accuracy: 0.8559, Val F1-score: 0.8504\n\nEpoch 54, Loss: 0.31488729822329986\nValidation Loss: 0.31609509587287904\nVal Accuracy: 0.8559, Val F1-score: 0.8504\n\nEpoch 55, Loss: 0.3148780915981684\nValidation Loss: 0.31608734130859373\nVal Accuracy: 0.8560, Val F1-score: 0.8505\n\nEpoch 56, Loss: 0.3148696819941203\nValidation Loss: 0.31608041524887087\nVal Accuracy: 0.8559, Val F1-score: 0.8504\n\nEpoch 57, Loss: 0.3148619555510007\nValidation Loss: 0.3160742253065109\nVal Accuracy: 0.8560, Val F1-score: 0.8505\n\nEpoch 58, Loss: 0.3148548159843836\nValidation Loss: 0.31606863141059877\nVal Accuracy: 0.8560, Val F1-score: 0.8505\n\nEpoch 59, Loss: 0.3148481616607079\nValidation Loss: 0.31606360375881193\nVal Accuracy: 0.8560, Val F1-score: 0.8505\n\nEpoch 60, Loss: 0.3148419314470047\nValidation Loss: 0.3160590618848801\nVal Accuracy: 0.8560, Val F1-score: 0.8505\n\nEpoch 61, Loss: 0.3148360573328458\nValidation Loss: 0.3160549283027649\nVal Accuracy: 0.8558, Val F1-score: 0.8503\n\nEpoch 62, Loss: 0.31483048506272143\nValidation Loss: 0.31605119407176974\nVal Accuracy: 0.8558, Val F1-score: 0.8503\n\nEpoch 63, Loss: 0.31482517642852587\nValidation Loss: 0.31604780852794645\nVal Accuracy: 0.8557, Val F1-score: 0.8502\n\nEpoch 64, Loss: 0.31482009016550505\nValidation Loss: 0.3160447031259537\nVal Accuracy: 0.8557, Val F1-score: 0.8502\n\nEpoch 65, Loss: 0.31481519494301236\nValidation Loss: 0.3160418957471848\nVal Accuracy: 0.8557, Val F1-score: 0.8502\n\nEpoch 66, Loss: 0.31481046477953595\nValidation Loss: 0.3160393089056015\nVal Accuracy: 0.8557, Val F1-score: 0.8502\n\nEpoch 67, Loss: 0.31480589050513047\nValidation Loss: 0.3160369545221329\nVal Accuracy: 0.8557, Val F1-score: 0.8502\n\nEpoch 68, Loss: 0.31480143544001454\nValidation Loss: 0.31603482365608215\nVal Accuracy: 0.8558, Val F1-score: 0.8503\n\nEpoch 69, Loss: 0.3147971034049988\nValidation Loss: 0.3160328656435013\nVal Accuracy: 0.8558, Val F1-score: 0.8503\n\nEpoch 70, Loss: 0.3147928584844638\nValidation Loss: 0.3160310834646225\nVal Accuracy: 0.8558, Val F1-score: 0.8503\n\nEpoch 71, Loss: 0.31478871748997617\nValidation Loss: 0.31602945923805237\nVal Accuracy: 0.8557, Val F1-score: 0.8502\n\nEpoch 72, Loss: 0.31478466131748295\nValidation Loss: 0.3160280019044876\nVal Accuracy: 0.8557, Val F1-score: 0.8502\n\nEpoch 73, Loss: 0.3147806731554178\nValidation Loss: 0.3160266697406769\nVal Accuracy: 0.8557, Val F1-score: 0.8502\n\nEpoch 74, Loss: 0.314776755296267\nValidation Loss: 0.31602545976638796\nVal Accuracy: 0.8557, Val F1-score: 0.8502\n\nEpoch 75, Loss: 0.31477291079667896\nValidation Loss: 0.31602436006069184\nVal Accuracy: 0.8557, Val F1-score: 0.8502\n\nEpoch 76, Loss: 0.31476912437341154\nValidation Loss: 0.31602340638637544\nVal Accuracy: 0.8557, Val F1-score: 0.8502\n\nEpoch 77, Loss: 0.31476540519640994\nValidation Loss: 0.3160225450992584\nVal Accuracy: 0.8557, Val F1-score: 0.8502\n\nEpoch 78, Loss: 0.3147617349257836\nValidation Loss: 0.3160217821598053\nVal Accuracy: 0.8557, Val F1-score: 0.8502\n\nEpoch 79, Loss: 0.31475813419390947\nValidation Loss: 0.3160211265087128\nVal Accuracy: 0.8557, Val F1-score: 0.8502\n\nEpoch 80, Loss: 0.31475457549095154\nValidation Loss: 0.3160205513238907\nVal Accuracy: 0.8557, Val F1-score: 0.8502\n\nEpoch 81, Loss: 0.31475106875101727\nValidation Loss: 0.3160200506448746\nVal Accuracy: 0.8557, Val F1-score: 0.8502\n\nEpoch 82, Loss: 0.3147476216157277\nValidation Loss: 0.3160196363925934\nVal Accuracy: 0.8557, Val F1-score: 0.8502\n\nEpoch 83, Loss: 0.3147442218584892\nValidation Loss: 0.3160192996263504\nVal Accuracy: 0.8557, Val F1-score: 0.8502\n\nEpoch 84, Loss: 0.31474087024346376\nValidation Loss: 0.31601903438568113\nVal Accuracy: 0.8557, Val F1-score: 0.8502\n\nEpoch 85, Loss: 0.31473757059146196\nValidation Loss: 0.3160188227891922\nVal Accuracy: 0.8557, Val F1-score: 0.8502\n\nEpoch 86, Loss: 0.3147343229024838\nValidation Loss: 0.3160186767578125\nVal Accuracy: 0.8558, Val F1-score: 0.8503\n\nEpoch 87, Loss: 0.31473111418577343\nValidation Loss: 0.31601859629154205\nVal Accuracy: 0.8558, Val F1-score: 0.8503\n\nEpoch 88, Loss: 0.3147279589604109\nValidation Loss: 0.3160185873508453\nVal Accuracy: 0.8558, Val F1-score: 0.8503\n\nEpoch 89, Loss: 0.3147248526414235\nValidation Loss: 0.3160186141729355\nVal Accuracy: 0.8558, Val F1-score: 0.8503\n\nEpoch 90, Loss: 0.31472178987967664\nValidation Loss: 0.31601869165897367\nVal Accuracy: 0.8558, Val F1-score: 0.8503\n\nEpoch 91, Loss: 0.3147187691468459\nValidation Loss: 0.31601881682872773\nVal Accuracy: 0.8558, Val F1-score: 0.8503\n\nEpoch 92, Loss: 0.3147158041978494\nValidation Loss: 0.31601898074150087\nVal Accuracy: 0.8558, Val F1-score: 0.8503\n\nEpoch 93, Loss: 0.3147128774569585\nValidation Loss: 0.31601919829845426\nVal Accuracy: 0.8558, Val F1-score: 0.8503\n\nEarly stopping triggered. Best validation loss at epoch 88: 0.3160\nVal Accuracy: 0.8558\nEarly Train Loss: 0.3147279589604109\n","output_type":"stream"}]},{"cell_type":"code","source":"X_tensor = torch.tensor(x_scaled).float()\n\nx_scaled_1 = None\nraw_output = None\n\nwith torch.no_grad():\n    raw_output = model(X_tensor)\n    raw_output_np = raw_output.detach().numpy()\n        \n    x_scaled_1 = np.concatenate((x_scaled, raw_output_np), axis=1)\n    x_scaled_1 = x_scaler.fit_transform(x_scaled_1)\n    \nprint(raw_output.shape)\nprint(x_scaled_1.shape)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T04:11:41.690648Z","iopub.execute_input":"2024-10-02T04:11:41.691076Z","iopub.status.idle":"2024-10-02T04:11:41.836832Z","shell.execute_reply.started":"2024-10-02T04:11:41.691037Z","shell.execute_reply":"2024-10-02T04:11:41.835781Z"},"trusted":true},"execution_count":548,"outputs":[{"name":"stdout","text":"torch.Size([48842, 2])\n(48842, 102)\n","output_type":"stream"}]},{"cell_type":"code","source":"num_features = 102\n\nmodel1 = DenseGrowNet(num_features, num_classes, False).to(device)\ncustom_train_loader = CustomDataLoader(x_scaled_1, y_encoded, batch_size=1024, validation_size=0.2, raw_output=raw_output)\n\noptimizer = optim.Adam(model1.parameters(), lr=0.0001)\nevaluate_model(model1, custom_train_loader, criterion, optimizer, num_epochs, num_features, 1024, patience=5)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T04:11:42.485346Z","iopub.execute_input":"2024-10-02T04:11:42.486237Z","iopub.status.idle":"2024-10-02T04:11:53.175884Z","shell.execute_reply.started":"2024-10-02T04:11:42.486187Z","shell.execute_reply":"2024-10-02T04:11:53.174787Z"},"trusted":true},"execution_count":549,"outputs":[{"name":"stdout","text":"torch.Size([9769, 2])\nEpoch 1, Loss: 0.3136664529641469\nValidation Loss: 0.3159541130065918\nVal Accuracy: 0.8561, Val F1-score: 0.8507\n\ntorch.Size([9769, 2])\nEpoch 2, Loss: 0.3135322561630836\nValidation Loss: 0.3158873528242111\nVal Accuracy: 0.8562, Val F1-score: 0.8507\n\ntorch.Size([9769, 2])\nEpoch 3, Loss: 0.3134492161946419\nValidation Loss: 0.31581225991249084\nVal Accuracy: 0.8566, Val F1-score: 0.8512\n\ntorch.Size([9769, 2])\nEpoch 4, Loss: 0.31335888917629534\nValidation Loss: 0.3157337337732315\nVal Accuracy: 0.8563, Val F1-score: 0.8509\n\ntorch.Size([9769, 2])\nEpoch 5, Loss: 0.3132594365340013\nValidation Loss: 0.3156507819890976\nVal Accuracy: 0.8562, Val F1-score: 0.8508\n\ntorch.Size([9769, 2])\nEpoch 6, Loss: 0.31314825247495603\nValidation Loss: 0.3155656546354294\nVal Accuracy: 0.8560, Val F1-score: 0.8506\n\ntorch.Size([9769, 2])\nEpoch 7, Loss: 0.3130245560254806\nValidation Loss: 0.3154775172472\nVal Accuracy: 0.8560, Val F1-score: 0.8507\n\ntorch.Size([9769, 2])\nEpoch 8, Loss: 0.31288860929317963\nValidation Loss: 0.3153871238231659\nVal Accuracy: 0.8560, Val F1-score: 0.8507\n\ntorch.Size([9769, 2])\nEpoch 9, Loss: 0.3127414882183075\nValidation Loss: 0.31529186964035033\nVal Accuracy: 0.8560, Val F1-score: 0.8507\n\ntorch.Size([9769, 2])\nEpoch 10, Loss: 0.31258330054772204\nValidation Loss: 0.3151948720216751\nVal Accuracy: 0.8561, Val F1-score: 0.8508\n\ntorch.Size([9769, 2])\nEpoch 11, Loss: 0.31241352206621414\nValidation Loss: 0.3150945007801056\nVal Accuracy: 0.8560, Val F1-score: 0.8507\n\ntorch.Size([9769, 2])\nEpoch 12, Loss: 0.3122333586215973\nValidation Loss: 0.3149898797273636\nVal Accuracy: 0.8556, Val F1-score: 0.8503\n\ntorch.Size([9769, 2])\nEpoch 13, Loss: 0.3120396863191556\nValidation Loss: 0.3148806244134903\nVal Accuracy: 0.8555, Val F1-score: 0.8502\n\ntorch.Size([9769, 2])\nEpoch 14, Loss: 0.31183089506931794\nValidation Loss: 0.31476386189460753\nVal Accuracy: 0.8554, Val F1-score: 0.8501\n\ntorch.Size([9769, 2])\nEpoch 15, Loss: 0.3116050156263205\nValidation Loss: 0.31463488936424255\nVal Accuracy: 0.8553, Val F1-score: 0.8500\n\ntorch.Size([9769, 2])\nEpoch 16, Loss: 0.3113557535868425\nValidation Loss: 0.3144941121339798\nVal Accuracy: 0.8554, Val F1-score: 0.8501\n\ntorch.Size([9769, 2])\nEpoch 17, Loss: 0.31108256257497346\nValidation Loss: 0.3143413424491882\nVal Accuracy: 0.8555, Val F1-score: 0.8503\n\ntorch.Size([9769, 2])\nEpoch 18, Loss: 0.31078675847787124\nValidation Loss: 0.31417510509490965\nVal Accuracy: 0.8556, Val F1-score: 0.8505\n\ntorch.Size([9769, 2])\nEpoch 19, Loss: 0.31047218503096163\nValidation Loss: 0.31400191485881807\nVal Accuracy: 0.8560, Val F1-score: 0.8509\n\ntorch.Size([9769, 2])\nEpoch 20, Loss: 0.3101429778795976\nValidation Loss: 0.3138254642486572\nVal Accuracy: 0.8560, Val F1-score: 0.8509\n\ntorch.Size([9769, 2])\nEpoch 21, Loss: 0.3098003107767839\nValidation Loss: 0.31364282965660095\nVal Accuracy: 0.8559, Val F1-score: 0.8508\n\ntorch.Size([9769, 2])\nEpoch 22, Loss: 0.3094470019523914\nValidation Loss: 0.31344722509384154\nVal Accuracy: 0.8555, Val F1-score: 0.8503\n\ntorch.Size([9769, 2])\nEpoch 23, Loss: 0.30908557772636414\nValidation Loss: 0.3132564127445221\nVal Accuracy: 0.8554, Val F1-score: 0.8502\n\ntorch.Size([9769, 2])\nEpoch 24, Loss: 0.3087172706921895\nValidation Loss: 0.31306238770484923\nVal Accuracy: 0.8552, Val F1-score: 0.8500\n\ntorch.Size([9769, 2])\nEpoch 25, Loss: 0.3083437184492747\nValidation Loss: 0.312861493229866\nVal Accuracy: 0.8555, Val F1-score: 0.8504\n\ntorch.Size([9769, 2])\nEpoch 26, Loss: 0.30796706905731785\nValidation Loss: 0.3126664638519287\nVal Accuracy: 0.8557, Val F1-score: 0.8506\n\ntorch.Size([9769, 2])\nEpoch 27, Loss: 0.30758825555825847\nValidation Loss: 0.31247068345546725\nVal Accuracy: 0.8555, Val F1-score: 0.8503\n\ntorch.Size([9769, 2])\nEpoch 28, Loss: 0.3072084998473143\nValidation Loss: 0.31227836608886717\nVal Accuracy: 0.8558, Val F1-score: 0.8506\n\ntorch.Size([9769, 2])\nEpoch 29, Loss: 0.30682710424447673\nValidation Loss: 0.3120861709117889\nVal Accuracy: 0.8563, Val F1-score: 0.8512\n\ntorch.Size([9769, 2])\nEpoch 30, Loss: 0.3064485864761548\nValidation Loss: 0.3119016408920288\nVal Accuracy: 0.8564, Val F1-score: 0.8513\n\ntorch.Size([9769, 2])\nEpoch 31, Loss: 0.30606842805177736\nValidation Loss: 0.3117192804813385\nVal Accuracy: 0.8568, Val F1-score: 0.8517\n\ntorch.Size([9769, 2])\nEpoch 32, Loss: 0.3056894365029457\nValidation Loss: 0.31153797507286074\nVal Accuracy: 0.8565, Val F1-score: 0.8515\n\ntorch.Size([9769, 2])\nEpoch 33, Loss: 0.30531249596522403\nValidation Loss: 0.3113720566034317\nVal Accuracy: 0.8571, Val F1-score: 0.8522\n\ntorch.Size([9769, 2])\nEpoch 34, Loss: 0.304939050705005\nValidation Loss: 0.31120761930942537\nVal Accuracy: 0.8575, Val F1-score: 0.8526\n\ntorch.Size([9769, 2])\nEpoch 35, Loss: 0.3045650285023909\nValidation Loss: 0.3110437035560608\nVal Accuracy: 0.8578, Val F1-score: 0.8530\n\ntorch.Size([9769, 2])\nEpoch 36, Loss: 0.3041964463698558\nValidation Loss: 0.3108910292387009\nVal Accuracy: 0.8579, Val F1-score: 0.8532\n\ntorch.Size([9769, 2])\nEpoch 37, Loss: 0.30382935741008854\nValidation Loss: 0.3107353985309601\nVal Accuracy: 0.8580, Val F1-score: 0.8533\n\ntorch.Size([9769, 2])\nEpoch 38, Loss: 0.3034663024621132\nValidation Loss: 0.3105955570936203\nVal Accuracy: 0.8584, Val F1-score: 0.8537\n\ntorch.Size([9769, 2])\nEpoch 39, Loss: 0.3031077774671408\nValidation Loss: 0.3104575663805008\nVal Accuracy: 0.8589, Val F1-score: 0.8542\n\ntorch.Size([9769, 2])\nEpoch 40, Loss: 0.3027512851433876\nValidation Loss: 0.3103255659341812\nVal Accuracy: 0.8592, Val F1-score: 0.8545\n\ntorch.Size([9769, 2])\nEpoch 41, Loss: 0.30239808635833937\nValidation Loss: 0.3101984977722168\nVal Accuracy: 0.8590, Val F1-score: 0.8543\n\ntorch.Size([9769, 2])\nEpoch 42, Loss: 0.30205081976377046\nValidation Loss: 0.3100805163383484\nVal Accuracy: 0.8591, Val F1-score: 0.8543\n\ntorch.Size([9769, 2])\nEpoch 43, Loss: 0.3017075321613214\nValidation Loss: 0.30996633768081666\nVal Accuracy: 0.8597, Val F1-score: 0.8549\n\ntorch.Size([9769, 2])\nEpoch 44, Loss: 0.3013680348029503\nValidation Loss: 0.3098524421453476\nVal Accuracy: 0.8595, Val F1-score: 0.8547\n\ntorch.Size([9769, 2])\nEpoch 45, Loss: 0.30103270671306515\nValidation Loss: 0.30974688827991487\nVal Accuracy: 0.8594, Val F1-score: 0.8546\n\ntorch.Size([9769, 2])\nEpoch 46, Loss: 0.3007005430184878\nValidation Loss: 0.3096481800079346\nVal Accuracy: 0.8592, Val F1-score: 0.8544\n\ntorch.Size([9769, 2])\nEpoch 47, Loss: 0.3003732401591081\nValidation Loss: 0.30955336689949037\nVal Accuracy: 0.8594, Val F1-score: 0.8545\n\ntorch.Size([9769, 2])\nEpoch 48, Loss: 0.30005034575095546\nValidation Loss: 0.3094672441482544\nVal Accuracy: 0.8596, Val F1-score: 0.8547\n\ntorch.Size([9769, 2])\nEpoch 49, Loss: 0.29973332316447526\nValidation Loss: 0.30938261449337007\nVal Accuracy: 0.8594, Val F1-score: 0.8545\n\ntorch.Size([9769, 2])\nEpoch 50, Loss: 0.29941939008541596\nValidation Loss: 0.30929888784885406\nVal Accuracy: 0.8595, Val F1-score: 0.8547\n\ntorch.Size([9769, 2])\nEpoch 51, Loss: 0.29911334621600616\nValidation Loss: 0.30923002660274507\nVal Accuracy: 0.8591, Val F1-score: 0.8543\n\ntorch.Size([9769, 2])\nEpoch 52, Loss: 0.2988086610268324\nValidation Loss: 0.3091692507266998\nVal Accuracy: 0.8588, Val F1-score: 0.8540\n\ntorch.Size([9769, 2])\nEpoch 53, Loss: 0.29851044370577884\nValidation Loss: 0.30910117626190187\nVal Accuracy: 0.8586, Val F1-score: 0.8538\n\ntorch.Size([9769, 2])\nEpoch 54, Loss: 0.2982164109364534\nValidation Loss: 0.3090445280075073\nVal Accuracy: 0.8582, Val F1-score: 0.8534\n\ntorch.Size([9769, 2])\nEpoch 55, Loss: 0.29792597584235364\nValidation Loss: 0.30899216532707213\nVal Accuracy: 0.8589, Val F1-score: 0.8542\n\ntorch.Size([9769, 2])\nEpoch 56, Loss: 0.29763856300940883\nValidation Loss: 0.3089457005262375\nVal Accuracy: 0.8587, Val F1-score: 0.8540\n\ntorch.Size([9769, 2])\nEpoch 57, Loss: 0.2973575202318338\nValidation Loss: 0.3088973432779312\nVal Accuracy: 0.8588, Val F1-score: 0.8541\n\ntorch.Size([9769, 2])\nEpoch 58, Loss: 0.2970767242786212\nValidation Loss: 0.3088646501302719\nVal Accuracy: 0.8588, Val F1-score: 0.8540\n\ntorch.Size([9769, 2])\nEpoch 59, Loss: 0.2968007761698503\nValidation Loss: 0.3088281095027924\nVal Accuracy: 0.8588, Val F1-score: 0.8540\n\ntorch.Size([9769, 2])\nEpoch 60, Loss: 0.2965295330072061\nValidation Loss: 0.3087912231683731\nVal Accuracy: 0.8591, Val F1-score: 0.8543\n\ntorch.Size([9769, 2])\nEpoch 61, Loss: 0.2962613717103616\nValidation Loss: 0.30876325964927676\nVal Accuracy: 0.8591, Val F1-score: 0.8543\n\ntorch.Size([9769, 2])\nEpoch 62, Loss: 0.29599722226460773\nValidation Loss: 0.3087434023618698\nVal Accuracy: 0.8594, Val F1-score: 0.8546\n\ntorch.Size([9769, 2])\nEpoch 63, Loss: 0.2957366307576497\nValidation Loss: 0.3087290197610855\nVal Accuracy: 0.8592, Val F1-score: 0.8545\n\ntorch.Size([9769, 2])\nEpoch 64, Loss: 0.29548218234991414\nValidation Loss: 0.3087203085422516\nVal Accuracy: 0.8594, Val F1-score: 0.8546\n\ntorch.Size([9769, 2])\nEpoch 65, Loss: 0.2952284606603476\nValidation Loss: 0.30870783030986787\nVal Accuracy: 0.8592, Val F1-score: 0.8545\n\ntorch.Size([9769, 2])\nEpoch 66, Loss: 0.2949791115063887\nValidation Loss: 0.30870757102966306\nVal Accuracy: 0.8596, Val F1-score: 0.8549\n\ntorch.Size([9769, 2])\nEpoch 67, Loss: 0.2947337382879013\nValidation Loss: 0.3087117850780487\nVal Accuracy: 0.8595, Val F1-score: 0.8548\n\ntorch.Size([9769, 2])\nEpoch 68, Loss: 0.29449164867401123\nValidation Loss: 0.30871590673923494\nVal Accuracy: 0.8591, Val F1-score: 0.8544\n\ntorch.Size([9769, 2])\nEpoch 69, Loss: 0.29425206551185024\nValidation Loss: 0.3087137401103973\nVal Accuracy: 0.8590, Val F1-score: 0.8543\n\ntorch.Size([9769, 2])\nEpoch 70, Loss: 0.2940184091910338\nValidation Loss: 0.3087218850851059\nVal Accuracy: 0.8590, Val F1-score: 0.8543\n\ntorch.Size([9769, 2])\nEpoch 71, Loss: 0.2937858861226302\nValidation Loss: 0.30873238742351533\nVal Accuracy: 0.8592, Val F1-score: 0.8545\n\nEarly stopping triggered. Best validation loss at epoch 66: 0.3087\nVal Accuracy: 0.8596\nEarly Train Loss: 0.2949791115063887\n","output_type":"stream"}]},{"cell_type":"code","source":"X_tensor_1 = torch.tensor(x_scaled_1).float()\nprint(X_tensor_1.shape)\n\nx_scaled_2 = None\nraw_output_1 = None\n\nwith torch.no_grad():\n    raw_output_1 = model1(X_tensor_1, raw_output)\n    print(raw_output_1.shape)\n    raw_output_np = raw_output.detach().numpy()\n        \n    x_scaled_2 = np.concatenate((x_scaled_1, raw_output_np), axis=1)\n    x_scaled_2 = x_scaler.fit_transform(x_scaled_2)\n    \nprint(x_scaled_2.shape)\nprint(raw_output_1.shape)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T04:13:06.902267Z","iopub.execute_input":"2024-10-02T04:13:06.902698Z","iopub.status.idle":"2024-10-02T04:13:07.107590Z","shell.execute_reply.started":"2024-10-02T04:13:06.902657Z","shell.execute_reply":"2024-10-02T04:13:07.106339Z"},"trusted":true},"execution_count":551,"outputs":[{"name":"stdout","text":"torch.Size([48842, 102])\ntorch.Size([48842, 2])\n(48842, 104)\ntorch.Size([48842, 2])\n","output_type":"stream"}]},{"cell_type":"code","source":"num_features = 104\n\nmodel2 = DenseGrowNet(num_features, num_classes, False).to(device)\ncustom_train_loader = CustomDataLoader(x_scaled_2, y_encoded, batch_size=1024, validation_size=0.2, raw_output=raw_output_1)\n\noptimizer = optim.Adam(model2.parameters(), lr=0.00006)\nevaluate_model(model2, custom_train_loader, criterion, optimizer, num_epochs, num_features, 1024, patience=5)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T04:17:27.536819Z","iopub.execute_input":"2024-10-02T04:17:27.538031Z","iopub.status.idle":"2024-10-02T04:17:41.087242Z","shell.execute_reply.started":"2024-10-02T04:17:27.537969Z","shell.execute_reply":"2024-10-02T04:17:41.085973Z"},"trusted":true},"execution_count":568,"outputs":[{"name":"stdout","text":"torch.Size([9769, 2])\nEpoch 1, Loss: 0.29336777787942153\nValidation Loss: 0.3087498605251312\nVal Accuracy: 0.8590, Val F1-score: 0.8543\n\ntorch.Size([9769, 2])\nEpoch 2, Loss: 0.2933294008939694\nValidation Loss: 0.30874393582344056\nVal Accuracy: 0.8591, Val F1-score: 0.8543\n\ntorch.Size([9769, 2])\nEpoch 3, Loss: 0.2933221535804944\nValidation Loss: 0.30873301029205324\nVal Accuracy: 0.8590, Val F1-score: 0.8542\n\ntorch.Size([9769, 2])\nEpoch 4, Loss: 0.2933142093511728\nValidation Loss: 0.3087207943201065\nVal Accuracy: 0.8590, Val F1-score: 0.8542\n\ntorch.Size([9769, 2])\nEpoch 5, Loss: 0.2933058242003123\nValidation Loss: 0.3087082266807556\nVal Accuracy: 0.8590, Val F1-score: 0.8542\n\ntorch.Size([9769, 2])\nEpoch 6, Loss: 0.2932970157036415\nValidation Loss: 0.3086954325437546\nVal Accuracy: 0.8589, Val F1-score: 0.8541\n\ntorch.Size([9769, 2])\nEpoch 7, Loss: 0.2932876623593844\nValidation Loss: 0.3086824119091034\nVal Accuracy: 0.8590, Val F1-score: 0.8542\n\ntorch.Size([9769, 2])\nEpoch 8, Loss: 0.2932777328368945\nValidation Loss: 0.30866954624652865\nVal Accuracy: 0.8590, Val F1-score: 0.8542\n\ntorch.Size([9769, 2])\nEpoch 9, Loss: 0.2932672462402246\nValidation Loss: 0.3086567759513855\nVal Accuracy: 0.8590, Val F1-score: 0.8542\n\ntorch.Size([9769, 2])\nEpoch 10, Loss: 0.293256245362453\nValidation Loss: 0.3086441159248352\nVal Accuracy: 0.8591, Val F1-score: 0.8543\n\ntorch.Size([9769, 2])\nEpoch 11, Loss: 0.2932447836949275\nValidation Loss: 0.3086322844028473\nVal Accuracy: 0.8591, Val F1-score: 0.8543\n\ntorch.Size([9769, 2])\nEpoch 12, Loss: 0.2932327489058177\nValidation Loss: 0.308620297908783\nVal Accuracy: 0.8590, Val F1-score: 0.8542\n\ntorch.Size([9769, 2])\nEpoch 13, Loss: 0.2932201004945315\nValidation Loss: 0.30860746204853057\nVal Accuracy: 0.8590, Val F1-score: 0.8542\n\ntorch.Size([9769, 2])\nEpoch 14, Loss: 0.2932069767744113\nValidation Loss: 0.3085942566394806\nVal Accuracy: 0.8589, Val F1-score: 0.8541\n\ntorch.Size([9769, 2])\nEpoch 15, Loss: 0.2931933915003752\nValidation Loss: 0.30858078598976135\nVal Accuracy: 0.8589, Val F1-score: 0.8541\n\ntorch.Size([9769, 2])\nEpoch 16, Loss: 0.29317933091750514\nValidation Loss: 0.30856673419475555\nVal Accuracy: 0.8590, Val F1-score: 0.8542\n\ntorch.Size([9769, 2])\nEpoch 17, Loss: 0.2931648179506644\nValidation Loss: 0.3085520327091217\nVal Accuracy: 0.8590, Val F1-score: 0.8542\n\ntorch.Size([9769, 2])\nEpoch 18, Loss: 0.29314975707958907\nValidation Loss: 0.30853726267814635\nVal Accuracy: 0.8590, Val F1-score: 0.8542\n\ntorch.Size([9769, 2])\nEpoch 19, Loss: 0.29313415441757595\nValidation Loss: 0.30852213203907014\nVal Accuracy: 0.8590, Val F1-score: 0.8542\n\ntorch.Size([9769, 2])\nEpoch 20, Loss: 0.2931182147600712\nValidation Loss: 0.3085065335035324\nVal Accuracy: 0.8589, Val F1-score: 0.8541\n\ntorch.Size([9769, 2])\nEpoch 21, Loss: 0.2931018318885412\nValidation Loss: 0.3084908455610275\nVal Accuracy: 0.8587, Val F1-score: 0.8539\n\ntorch.Size([9769, 2])\nEpoch 22, Loss: 0.29308515099378735\nValidation Loss: 0.3084750324487686\nVal Accuracy: 0.8587, Val F1-score: 0.8539\n\ntorch.Size([9769, 2])\nEpoch 23, Loss: 0.29306803987576413\nValidation Loss: 0.3084585130214691\nVal Accuracy: 0.8587, Val F1-score: 0.8539\n\ntorch.Size([9769, 2])\nEpoch 24, Loss: 0.2930506391403003\nValidation Loss: 0.30844186544418334\nVal Accuracy: 0.8588, Val F1-score: 0.8540\n\ntorch.Size([9769, 2])\nEpoch 25, Loss: 0.29303283798388946\nValidation Loss: 0.3084255486726761\nVal Accuracy: 0.8587, Val F1-score: 0.8539\n\ntorch.Size([9769, 2])\nEpoch 26, Loss: 0.29301449274405456\nValidation Loss: 0.30840919315814974\nVal Accuracy: 0.8588, Val F1-score: 0.8539\n\ntorch.Size([9769, 2])\nEpoch 27, Loss: 0.29299576847981185\nValidation Loss: 0.308393195271492\nVal Accuracy: 0.8588, Val F1-score: 0.8539\n\ntorch.Size([9769, 2])\nEpoch 28, Loss: 0.29297664303046006\nValidation Loss: 0.30837771892547605\nVal Accuracy: 0.8588, Val F1-score: 0.8539\n\ntorch.Size([9769, 2])\nEpoch 29, Loss: 0.2929571339717278\nValidation Loss: 0.3083627253770828\nVal Accuracy: 0.8589, Val F1-score: 0.8541\n\ntorch.Size([9769, 2])\nEpoch 30, Loss: 0.2929372313695076\nValidation Loss: 0.3083478152751923\nVal Accuracy: 0.8589, Val F1-score: 0.8541\n\ntorch.Size([9769, 2])\nEpoch 31, Loss: 0.29291680455207825\nValidation Loss: 0.3083334147930145\nVal Accuracy: 0.8589, Val F1-score: 0.8541\n\ntorch.Size([9769, 2])\nEpoch 32, Loss: 0.29289589249170744\nValidation Loss: 0.30831952691078185\nVal Accuracy: 0.8590, Val F1-score: 0.8542\n\ntorch.Size([9769, 2])\nEpoch 33, Loss: 0.2928744103664007\nValidation Loss: 0.3083061397075653\nVal Accuracy: 0.8590, Val F1-score: 0.8542\n\ntorch.Size([9769, 2])\nEpoch 34, Loss: 0.2928522504293002\nValidation Loss: 0.3082926481962204\nVal Accuracy: 0.8590, Val F1-score: 0.8542\n\ntorch.Size([9769, 2])\nEpoch 35, Loss: 0.29282929117862994\nValidation Loss: 0.30827951431274414\nVal Accuracy: 0.8590, Val F1-score: 0.8542\n\ntorch.Size([9769, 2])\nEpoch 36, Loss: 0.2928056877392989\nValidation Loss: 0.3082667887210846\nVal Accuracy: 0.8590, Val F1-score: 0.8542\n\ntorch.Size([9769, 2])\nEpoch 37, Loss: 0.29278131096791\nValidation Loss: 0.30825431644916534\nVal Accuracy: 0.8590, Val F1-score: 0.8542\n\ntorch.Size([9769, 2])\nEpoch 38, Loss: 0.29275611348641223\nValidation Loss: 0.3082423657178879\nVal Accuracy: 0.8590, Val F1-score: 0.8542\n\ntorch.Size([9769, 2])\nEpoch 39, Loss: 0.29273022825901324\nValidation Loss: 0.30823069214820864\nVal Accuracy: 0.8590, Val F1-score: 0.8542\n\ntorch.Size([9769, 2])\nEpoch 40, Loss: 0.2927034917550209\nValidation Loss: 0.3082195699214935\nVal Accuracy: 0.8591, Val F1-score: 0.8543\n\ntorch.Size([9769, 2])\nEpoch 41, Loss: 0.29267593606924397\nValidation Loss: 0.3082082450389862\nVal Accuracy: 0.8592, Val F1-score: 0.8544\n\ntorch.Size([9769, 2])\nEpoch 42, Loss: 0.2926476781184857\nValidation Loss: 0.30819720923900606\nVal Accuracy: 0.8592, Val F1-score: 0.8544\n\ntorch.Size([9769, 2])\nEpoch 43, Loss: 0.29261855360789174\nValidation Loss: 0.30818556249141693\nVal Accuracy: 0.8592, Val F1-score: 0.8544\n\ntorch.Size([9769, 2])\nEpoch 44, Loss: 0.29258901797808134\nValidation Loss: 0.3081738710403442\nVal Accuracy: 0.8592, Val F1-score: 0.8544\n\ntorch.Size([9769, 2])\nEpoch 45, Loss: 0.2925585867502751\nValidation Loss: 0.3081630051136017\nVal Accuracy: 0.8595, Val F1-score: 0.8546\n\ntorch.Size([9769, 2])\nEpoch 46, Loss: 0.2925277986587622\nValidation Loss: 0.30815168619155886\nVal Accuracy: 0.8596, Val F1-score: 0.8547\n\ntorch.Size([9769, 2])\nEpoch 47, Loss: 0.29249611496925354\nValidation Loss: 0.30814134180545805\nVal Accuracy: 0.8596, Val F1-score: 0.8547\n\ntorch.Size([9769, 2])\nEpoch 48, Loss: 0.2924638497523772\nValidation Loss: 0.3081307768821716\nVal Accuracy: 0.8600, Val F1-score: 0.8551\n\ntorch.Size([9769, 2])\nEpoch 49, Loss: 0.2924308669872773\nValidation Loss: 0.30812107026576996\nVal Accuracy: 0.8603, Val F1-score: 0.8554\n\ntorch.Size([9769, 2])\nEpoch 50, Loss: 0.29239756174576587\nValidation Loss: 0.30811302959918974\nVal Accuracy: 0.8603, Val F1-score: 0.8554\n\ntorch.Size([9769, 2])\nEpoch 51, Loss: 0.2923635435410035\nValidation Loss: 0.30810381174087526\nVal Accuracy: 0.8603, Val F1-score: 0.8554\n\ntorch.Size([9769, 2])\nEpoch 52, Loss: 0.29232889337417406\nValidation Loss: 0.30809494853019714\nVal Accuracy: 0.8602, Val F1-score: 0.8554\n\ntorch.Size([9769, 2])\nEpoch 53, Loss: 0.2922939719297947\nValidation Loss: 0.30808613300323484\nVal Accuracy: 0.8602, Val F1-score: 0.8554\n\ntorch.Size([9769, 2])\nEpoch 54, Loss: 0.2922584307499421\nValidation Loss: 0.3080784320831299\nVal Accuracy: 0.8603, Val F1-score: 0.8555\n\ntorch.Size([9769, 2])\nEpoch 55, Loss: 0.2922224547618475\nValidation Loss: 0.3080706000328064\nVal Accuracy: 0.8605, Val F1-score: 0.8557\n\ntorch.Size([9769, 2])\nEpoch 56, Loss: 0.2921858697365492\nValidation Loss: 0.30806225836277007\nVal Accuracy: 0.8605, Val F1-score: 0.8557\n\ntorch.Size([9769, 2])\nEpoch 57, Loss: 0.29214871388215286\nValidation Loss: 0.30805434882640836\nVal Accuracy: 0.8606, Val F1-score: 0.8558\n\ntorch.Size([9769, 2])\nEpoch 58, Loss: 0.2921110307558989\nValidation Loss: 0.3080475389957428\nVal Accuracy: 0.8610, Val F1-score: 0.8563\n\ntorch.Size([9769, 2])\nEpoch 59, Loss: 0.29207247419235033\nValidation Loss: 0.30804034173488615\nVal Accuracy: 0.8610, Val F1-score: 0.8563\n\ntorch.Size([9769, 2])\nEpoch 60, Loss: 0.2920336196055779\nValidation Loss: 0.3080323874950409\nVal Accuracy: 0.8612, Val F1-score: 0.8565\n\ntorch.Size([9769, 2])\nEpoch 61, Loss: 0.29199393743123764\nValidation Loss: 0.30802554786205294\nVal Accuracy: 0.8612, Val F1-score: 0.8565\n\ntorch.Size([9769, 2])\nEpoch 62, Loss: 0.2919536324647757\nValidation Loss: 0.30801973938941957\nVal Accuracy: 0.8614, Val F1-score: 0.8567\n\ntorch.Size([9769, 2])\nEpoch 63, Loss: 0.29191267108305907\nValidation Loss: 0.3080128371715546\nVal Accuracy: 0.8614, Val F1-score: 0.8567\n\ntorch.Size([9769, 2])\nEpoch 64, Loss: 0.2918712061185103\nValidation Loss: 0.30800761580467223\nVal Accuracy: 0.8614, Val F1-score: 0.8567\n\ntorch.Size([9769, 2])\nEpoch 65, Loss: 0.29182922228788716\nValidation Loss: 0.308002832531929\nVal Accuracy: 0.8611, Val F1-score: 0.8564\n\ntorch.Size([9769, 2])\nEpoch 66, Loss: 0.291786375718239\nValidation Loss: 0.3079977482557297\nVal Accuracy: 0.8611, Val F1-score: 0.8564\n\ntorch.Size([9769, 2])\nEpoch 67, Loss: 0.29174293463046735\nValidation Loss: 0.3079925268888474\nVal Accuracy: 0.8611, Val F1-score: 0.8564\n\ntorch.Size([9769, 2])\nEpoch 68, Loss: 0.29169912215990895\nValidation Loss: 0.30798697769641875\nVal Accuracy: 0.8612, Val F1-score: 0.8565\n\ntorch.Size([9769, 2])\nEpoch 69, Loss: 0.2916545264231853\nValidation Loss: 0.307982474565506\nVal Accuracy: 0.8612, Val F1-score: 0.8565\n\ntorch.Size([9769, 2])\nEpoch 70, Loss: 0.2916094530851413\nValidation Loss: 0.30797951221466063\nVal Accuracy: 0.8612, Val F1-score: 0.8565\n\ntorch.Size([9769, 2])\nEpoch 71, Loss: 0.29156374320005757\nValidation Loss: 0.30797587633132933\nVal Accuracy: 0.8612, Val F1-score: 0.8565\n\ntorch.Size([9769, 2])\nEpoch 72, Loss: 0.2915175717610579\nValidation Loss: 0.30797342360019686\nVal Accuracy: 0.8613, Val F1-score: 0.8566\n\ntorch.Size([9769, 2])\nEpoch 73, Loss: 0.2914707248027508\nValidation Loss: 0.307971265912056\nVal Accuracy: 0.8613, Val F1-score: 0.8566\n\ntorch.Size([9769, 2])\nEpoch 74, Loss: 0.29142345984776813\nValidation Loss: 0.3079687267541885\nVal Accuracy: 0.8612, Val F1-score: 0.8565\n\ntorch.Size([9769, 2])\nEpoch 75, Loss: 0.2913755430625035\nValidation Loss: 0.3079659312963486\nVal Accuracy: 0.8612, Val F1-score: 0.8565\n\ntorch.Size([9769, 2])\nEpoch 76, Loss: 0.2913268758700444\nValidation Loss: 0.30796392261981964\nVal Accuracy: 0.8612, Val F1-score: 0.8565\n\ntorch.Size([9769, 2])\nEpoch 77, Loss: 0.29127765466005373\nValidation Loss: 0.3079633593559265\nVal Accuracy: 0.8613, Val F1-score: 0.8566\n\ntorch.Size([9769, 2])\nEpoch 78, Loss: 0.2912278121862656\nValidation Loss: 0.30796014666557314\nVal Accuracy: 0.8614, Val F1-score: 0.8567\n\ntorch.Size([9769, 2])\nEpoch 79, Loss: 0.2911770206231337\nValidation Loss: 0.3079594671726227\nVal Accuracy: 0.8615, Val F1-score: 0.8568\n\ntorch.Size([9769, 2])\nEpoch 80, Loss: 0.291125937914237\nValidation Loss: 0.3079583287239075\nVal Accuracy: 0.8614, Val F1-score: 0.8567\n\ntorch.Size([9769, 2])\nEpoch 81, Loss: 0.29107355460142476\nValidation Loss: 0.3079571098089218\nVal Accuracy: 0.8612, Val F1-score: 0.8564\n\ntorch.Size([9769, 2])\nEpoch 82, Loss: 0.29102064974797076\nValidation Loss: 0.30795497000217437\nVal Accuracy: 0.8609, Val F1-score: 0.8561\n\ntorch.Size([9769, 2])\nEpoch 83, Loss: 0.290966850060683\nValidation Loss: 0.30795498192310333\nVal Accuracy: 0.8609, Val F1-score: 0.8562\n\ntorch.Size([9769, 2])\nEpoch 84, Loss: 0.2909125246298619\nValidation Loss: 0.3079537719488144\nVal Accuracy: 0.8609, Val F1-score: 0.8562\n\ntorch.Size([9769, 2])\nEpoch 85, Loss: 0.2908576776583989\nValidation Loss: 0.3079537868499756\nVal Accuracy: 0.8610, Val F1-score: 0.8563\n\ntorch.Size([9769, 2])\nEpoch 86, Loss: 0.2908020294629611\nValidation Loss: 0.30795290470123293\nVal Accuracy: 0.8609, Val F1-score: 0.8562\n\ntorch.Size([9769, 2])\nEpoch 87, Loss: 0.29074561519500536\nValidation Loss: 0.30795295238494874\nVal Accuracy: 0.8607, Val F1-score: 0.8560\n\ntorch.Size([9769, 2])\nEpoch 88, Loss: 0.2906888845639351\nValidation Loss: 0.30795426964759826\nVal Accuracy: 0.8607, Val F1-score: 0.8560\n\ntorch.Size([9769, 2])\nEpoch 89, Loss: 0.29063084645149034\nValidation Loss: 0.3079526275396347\nVal Accuracy: 0.8609, Val F1-score: 0.8562\n\ntorch.Size([9769, 2])\nEpoch 90, Loss: 0.29057285342461026\nValidation Loss: 0.30795338153839114\nVal Accuracy: 0.8609, Val F1-score: 0.8562\n\ntorch.Size([9769, 2])\nEpoch 91, Loss: 0.29051347879263073\nValidation Loss: 0.3079523414373398\nVal Accuracy: 0.8610, Val F1-score: 0.8563\n\ntorch.Size([9769, 2])\nEpoch 92, Loss: 0.29045371673046017\nValidation Loss: 0.30795349180698395\nVal Accuracy: 0.8608, Val F1-score: 0.8561\n\ntorch.Size([9769, 2])\nEpoch 93, Loss: 0.29039307970267075\nValidation Loss: 0.3079524040222168\nVal Accuracy: 0.8609, Val F1-score: 0.8562\n\ntorch.Size([9769, 2])\nEpoch 94, Loss: 0.2903316941780922\nValidation Loss: 0.307954004406929\nVal Accuracy: 0.8611, Val F1-score: 0.8565\n\ntorch.Size([9769, 2])\nEpoch 95, Loss: 0.2902696465070431\nValidation Loss: 0.3079531520605087\nVal Accuracy: 0.8611, Val F1-score: 0.8565\n\ntorch.Size([9769, 2])\nEpoch 96, Loss: 0.29020713919248337\nValidation Loss: 0.3079547047615051\nVal Accuracy: 0.8610, Val F1-score: 0.8563\n\nEarly stopping triggered. Best validation loss at epoch 91: 0.3080\nVal Accuracy: 0.8610\nEarly Train Loss: 0.29051347879263073\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}