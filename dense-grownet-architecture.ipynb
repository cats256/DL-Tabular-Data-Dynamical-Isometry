{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is recommended to read the explanation in README.md first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.datasets import california_housing\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import sys\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.datasets import mnist\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import logit\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.model_selection import LeaveOneOut, cross_val_predict, StratifiedKFold\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by https://arxiv.org/abs/1702.08591 (looks-linear init), https://arxiv.org/abs/1711.04735 (dynamical isometry), and https://openreview.net/forum?id=Fp7__phQszn (inductive bias of preserving original orientation of features with almost linear input-output mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLinearLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size, init=\"looks_linear\"):\n",
    "        super(CustomLinearLayer, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size, bias=True)\n",
    "        nn.init.zeros_(self.linear.bias)\n",
    "\n",
    "        if init == \"zero\":\n",
    "            nn.init.zeros_(self.linear.weight)\n",
    "        elif init == \"looks_linear\":\n",
    "            if input_size * 2 != output_size:\n",
    "                raise ValueError(\"Output size must be twice that of input size\")\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                weight = torch.zeros(input_size * 2, input_size)\n",
    "\n",
    "                for i in range(self.linear.in_features):\n",
    "                    weight[2 * i, i] = 1\n",
    "                    weight[2 * i + 1, i] = -1\n",
    "\n",
    "                self.linear.weight.copy_(weight)\n",
    "                nn.init.zeros_(self.linear.bias)\n",
    "                \n",
    "            \"\"\" Example matrix: [\n",
    "                [1, 0, 0],\n",
    "                [-1, 0, 0],\n",
    "                [0, 1, 0],\n",
    "                [0, -1, 0],\n",
    "                [0, 0, 1],\n",
    "                [0, 0, -1]\n",
    "            ] \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General idea of this approach is to start with a simple 1-layer model (no hidden layer or activation function) initialized to 0 weights. Train the model and use the predictions generated by the model as prev_output (see below) for the second model, which means the next model will essentially be training to fit the residuals between the predictions generated by the first model and the actual outcomes. The second model will be a two-layer model, where the first layer is initialized with looks-linear initialization and the second layer initialized to 0 weights, with the activation function after the first layer being ReLU. Train the second model and use the predictions generated by the second model as prev_output for the third model. Since the second model generate an intermediate features, where the number of intermediate features is 2 times the original features, we append the intermediate features to the list of features for use in the third model. Then, train the third model in similar fashing as second model. Rinse and repeat until validation loss of this approach no longer decreases.\n",
    "\n",
    "Example:\n",
    "- Train 1st model with 1-layer neural network (no hidden layer, essentially just doing linear regression), with zero weight initialization\n",
    "- Obtain the predictions generated by 1st model as prev_output for the second model\n",
    "- Adjust learning rate appropriately and train 2nd model with 2-layer neural network (input and output layer), where activation function is ReLU, with \"looks-linear\" initliazation for first layer and zero weight initialization for second layer\n",
    "- Obtain the features generated by the 1st layer of the 2nd model with self.extract_features()\n",
    "- Scale the features to zero mean and variance of one\n",
    "- Append the features to the feature list. The feature list will now be 3 times its original size as \"looks-linear\" initialization generate input_size * 2 amount of features\n",
    "- Train 3rd model in the same manner as the 2nd model, except the input features will be 3x the input features of 1st and 2nd model\n",
    "- Rinse and repeat until validation loss no longer decreases\n",
    "\n",
    "This architecture is independently discovered and is similar, but not the same, as this paper https://arxiv.org/abs/2002.07971. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseGrowNet(nn.Module):        \n",
    "    def __init__(self, input_size, output_size, is_first_model):\n",
    "        super(DenseGrowNet, self).__init__()\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.is_first_model = is_first_model\n",
    "                        \n",
    "        if is_first_model:\n",
    "            self.first_layer = CustomLinearLayer(input_size, output_size, init=\"zero\")\n",
    "        else:\n",
    "            self.first_layer = CustomLinearLayer(input_size, input_size * 2, init=\"looks_linear\")\n",
    "            self.last_layer = CustomLinearLayer(input_size * 2, output_size, init=\"zero\")\n",
    "\n",
    "    def forward(self, x, prev_output=None):\n",
    "        if self.is_first_model and prev_output is not None:\n",
    "            raise ValueError(\"This is the first model and prev_output is passed in\")\n",
    "        if not self.is_first_model and prev_output is None:\n",
    "            raise ValueError(\"This is not the first model and prev_output is not passed in\")\n",
    "        \n",
    "        if prev_output is None:\n",
    "            return self.first_layer(x)\n",
    "        \n",
    "        x = self.activation(self.first_layer(x))\n",
    "        return self.last_layer(x) + prev_output\n",
    "    \n",
    "    def extract_features(self, x, prev_output=None):\n",
    "        if self.is_first_model:\n",
    "            raise ValueError(\"This is not intended for the first model\")\n",
    "        if not self.is_first_model and prev_output is None:\n",
    "            raise ValueError(\"This is not the first model and prev_output is not passed in\")\n",
    "            \n",
    "        return self.activation(self.first_layer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Loss below with L1 and L2 regularization. It is only recommended to use with SGD optimizer. There may be issues with adaptive gradient algorithm when doing navive L1/L2 regularization as mentioned in this paper https://arxiv.org/abs/1711.05101 (not sure if AdamW actually works better in practice consensus wise). \n",
    "\n",
    "L1 regularization is particularly beneficial as it is rotationally invariant and robust against uninformative features, which are also the inductive biases that contribute to tree-based models strong performance on tabular data https://openreview.net/forum?id=Fp7__phQszn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, criterion, l1_lambda, l2_lambda):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.criterion = criterion\n",
    "        self.l1_lambda = l1_lambda\n",
    "        self.l2_lambda = l2_lambda\n",
    "\n",
    "    def forward(self, outputs, labels, model):\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        \n",
    "        l1_norm = sum(p.abs().sum() for name, p in model.named_parameters() if 'bias' not in name)\n",
    "        l2_norm = sum(p.pow(2.0).sum() for name, p in model.named_parameters() if 'bias' not in name)\n",
    "        \n",
    "        loss += self.l1_lambda * l1_norm + self.l2_lambda * l2_norm\n",
    "        return loss"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
